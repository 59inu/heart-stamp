import fs from 'fs';
import path from 'path';
import archiver from 'archiver';
import { S3Service } from './s3Service';
import { Pool } from 'pg';

const BACKUP_DIR = path.join(__dirname, '../../backups');
const UPLOADS_DIR = path.join(__dirname, '../../uploads');
const RETENTION_DAYS = 7; // 7ì¼ì¹˜ ë°±ì—… ë³´ê´€

// Railway í™˜ê²½ ê°ì§€
const IS_RAILWAY = process.env.RAILWAY_ENVIRONMENT !== undefined;

export class BackupService {
  /**
   * ë°±ì—… ë””ë ‰í† ë¦¬ ì´ˆê¸°í™” (ë¡œì»¬ í™˜ê²½ì—ë§Œ ì‚¬ìš©)
   */
  private static ensureBackupDir(): void {
    // RailwayëŠ” ephemeral filesystemì´ë¯€ë¡œ ë¡œì»¬ ì €ì¥ ë¶ˆí•„ìš”
    if (IS_RAILWAY) {
      console.log('âš ï¸  [Backup] Railway environment detected - skipping local backup dir');
      return;
    }

    if (!fs.existsSync(BACKUP_DIR)) {
      fs.mkdirSync(BACKUP_DIR, { recursive: true });
      console.log('âœ… [Backup] Created backup directory');
    }
  }

  /**
   * PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (pg ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© - pg_dump ë¶ˆí•„ìš”)
   */
  private static async performDatabaseBackup(timestamp: string): Promise<{ path: string; size: number }> {
    try {
      console.log('ğŸ“¦ [Backup] Starting database backup...');

      const databaseUrl = process.env.DATABASE_URL;
      if (!databaseUrl) {
        throw new Error('DATABASE_URL not configured');
      }

      // pg ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë°ì´í„° ì¶”ì¶œ
      const pool = new Pool({ connectionString: databaseUrl });

      try {
        // ëª¨ë“  í…Œì´ë¸” ì¡°íšŒ
        const tablesResult = await pool.query(`
          SELECT tablename
          FROM pg_tables
          WHERE schemaname = 'public'
        `);

        const tables = tablesResult.rows.map(row => row.tablename);
        console.log(`ğŸ“‹ [Backup] Found ${tables.length} tables: ${tables.join(', ')}`);

        // ê° í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
        const backupData: Record<string, any[]> = {};
        for (const table of tables) {
          const result = await pool.query(`SELECT * FROM "${table}"`);
          backupData[table] = result.rows;
          console.log(`   âœ“ ${table}: ${result.rows.length} rows`);
        }

        // JSON íŒŒì¼ë¡œ ì €ì¥
        const backupJson = JSON.stringify(backupData, null, 2);

        // Railway í™˜ê²½: ì„ì‹œ íŒŒì¼ë¡œë§Œ ìƒì„± (S3 ì—…ë¡œë“œ í›„ ì‚­ì œ ì˜ˆì •)
        // ë¡œì»¬ í™˜ê²½: backups/ ë””ë ‰í† ë¦¬ì— ì˜êµ¬ ì €ì¥
        const backupPath = IS_RAILWAY
          ? path.join('/tmp', `${timestamp}_diary_backup.json`)
          : path.join(BACKUP_DIR, `${timestamp}_diary_backup.json`);

        fs.writeFileSync(backupPath, backupJson);

        const stats = fs.statSync(backupPath);
        const sizeMB = (stats.size / 1024 / 1024).toFixed(2);

        console.log(`âœ… [Backup] Database backup created: ${sizeMB}MB (${IS_RAILWAY ? 'temp' : 'local'})`);

        return { path: backupPath, size: stats.size };
      } finally {
        await pool.end();
      }
    } catch (error) {
      console.error('âŒ [Backup] Database backup failed:', error);
      throw error;
    }
  }

  /**
   * uploads í´ë” ë°±ì—… (ZIP ì••ì¶•)
   */
  private static async performUploadsBackup(timestamp: string): Promise<{ path: string; size: number }> {
    return new Promise((resolve, reject) => {
      try {
        const backupPath = path.join(BACKUP_DIR, `${timestamp}_uploads.zip`);

        // uploads í´ë”ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if (!fs.existsSync(UPLOADS_DIR)) {
          console.log('âš ï¸  [Backup] No uploads directory found, skipping...');
          return resolve({ path: '', size: 0 });
        }

        // uploads í´ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
        const files = fs.readdirSync(UPLOADS_DIR);
        if (files.length === 0) {
          console.log('âš ï¸  [Backup] Uploads directory is empty, skipping...');
          return resolve({ path: '', size: 0 });
        }

        console.log('ğŸ“¦ [Backup] Starting uploads backup...');

        const output = fs.createWriteStream(backupPath);
        const archive = archiver('zip', { zlib: { level: 9 } }); // ìµœëŒ€ ì••ì¶•

        output.on('close', () => {
          const stats = fs.statSync(backupPath);
          const sizeMB = (stats.size / 1024 / 1024).toFixed(2);
          console.log(`âœ… [Backup] Uploads backup created: ${sizeMB}MB`);
          resolve({ path: backupPath, size: stats.size });
        });

        archive.on('error', (err) => {
          console.error('âŒ [Backup] Uploads backup failed:', err);
          reject(err);
        });

        archive.pipe(output);
        archive.directory(UPLOADS_DIR, false); // uploads/ í´ë” ë‚´ìš© ì••ì¶•
        archive.finalize();
      } catch (error) {
        console.error('âŒ [Backup] Uploads backup failed:', error);
        reject(error);
      }
    });
  }

  /**
   * ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ (RETENTION_DAYSë³´ë‹¤ ì˜¤ë˜ëœ íŒŒì¼)
   */
  private static cleanOldBackups(): void {
    try {
      if (!fs.existsSync(BACKUP_DIR)) return;

      const now = Date.now();
      const cutoffTime = now - RETENTION_DAYS * 24 * 60 * 60 * 1000;
      const files = fs.readdirSync(BACKUP_DIR);

      let deletedCount = 0;

      files.forEach((file) => {
        const filePath = path.join(BACKUP_DIR, file);
        const stats = fs.statSync(filePath);

        // JSON ë©”íƒ€ë°ì´í„° íŒŒì¼ê³¼ ë°±ì—… íŒŒì¼ ëª¨ë‘ ì‚­ì œ
        if (stats.mtimeMs < cutoffTime) {
          fs.unlinkSync(filePath);
          deletedCount++;
        }
      });

      if (deletedCount > 0) {
        console.log(`ğŸ—‘ï¸  [Backup] Deleted ${deletedCount} old backup file(s)`);
      }
    } catch (error) {
      console.error('âŒ [Backup] Failed to clean old backups:', error);
    }
  }


  /**
   * S3ì— ë°±ì—… íŒŒì¼ ì—…ë¡œë“œ
   */
  private static async uploadToS3(filePath: string, filename: string): Promise<void> {
    if (!S3Service.isConfigured()) {
      console.log('âš ï¸  [Backup] S3 not configured, skipping S3 upload');
      return;
    }

    try {
      const s3Key = `backups/${filename}`;
      await S3Service.uploadBackupFile(filePath, s3Key);
    } catch (error) {
      console.error(`âŒ [Backup] Failed to upload ${filename} to S3:`, error);
      // S3 ì—…ë¡œë“œ ì‹¤íŒ¨í•´ë„ ë¡œì»¬ ë°±ì—…ì€ ìœ ì§€ë˜ë¯€ë¡œ ì—ëŸ¬ ë˜ì§€ì§€ ì•ŠìŒ
    }
  }

  /**
   * ì „ì²´ ë°±ì—… ìˆ˜í–‰ (ë©”ì¸ í•¨ìˆ˜)
   */
  static async performFullBackup(): Promise<void> {
    const startTime = Date.now();
    const timestamp = new Date().toISOString().split('T')[0]; // 2025-11-03

    console.log(`\nğŸ”„ [Backup] Starting daily backup at ${new Date().toISOString()}`);

    try {
      // ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
      this.ensureBackupDir();

      // ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
      const dbBackup = await this.performDatabaseBackup(timestamp);

      // ì´ë¯¸ì§€ í´ë” ë°±ì—…
      const uploadsBackup = await this.performUploadsBackup(timestamp);

      // S3ì— ë°±ì—… ì—…ë¡œë“œ
      if (S3Service.isConfigured()) {
        console.log('ğŸ“¤ [Backup] Uploading backups to S3...');

        // DB ë°±ì—… ì—…ë¡œë“œ
        if (dbBackup.path) {
          await this.uploadToS3(dbBackup.path, path.basename(dbBackup.path));
          // Railway í™˜ê²½: ì„ì‹œ íŒŒì¼ ì‚­ì œ
          if (IS_RAILWAY && fs.existsSync(dbBackup.path)) {
            fs.unlinkSync(dbBackup.path);
            console.log('ğŸ—‘ï¸  [Backup] Cleaned up temp DB backup file');
          }
        }

        // uploads ë°±ì—… ì—…ë¡œë“œ
        if (uploadsBackup.path) {
          await this.uploadToS3(uploadsBackup.path, path.basename(uploadsBackup.path));
          // Railway í™˜ê²½: ì„ì‹œ íŒŒì¼ ì‚­ì œ
          if (IS_RAILWAY && fs.existsSync(uploadsBackup.path)) {
            fs.unlinkSync(uploadsBackup.path);
            console.log('ğŸ—‘ï¸  [Backup] Cleaned up temp uploads backup file');
          }
        }

        // ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì—…ë¡œë“œ
        const metadata = {
          timestamp,
          db_size_bytes: dbBackup.size,
          db_size_mb: (dbBackup.size / 1024 / 1024).toFixed(2),
          uploads_size_bytes: uploadsBackup.size,
          uploads_size_mb: (uploadsBackup.size / 1024 / 1024).toFixed(2),
          total_size_mb: ((dbBackup.size + uploadsBackup.size) / 1024 / 1024).toFixed(2),
          success: true,
          duration_seconds: ((Date.now() - startTime) / 1000).toFixed(2),
          environment: IS_RAILWAY ? 'railway' : 'local',
        };

        const metadataPath = IS_RAILWAY
          ? path.join('/tmp', `${timestamp}_metadata.json`)
          : path.join(BACKUP_DIR, `${timestamp}_metadata.json`);

        fs.writeFileSync(metadataPath, JSON.stringify(metadata, null, 2));
        await this.uploadToS3(metadataPath, `${timestamp}_metadata.json`);

        // Railway í™˜ê²½: ë©”íƒ€ë°ì´í„° ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if (IS_RAILWAY && fs.existsSync(metadataPath)) {
          fs.unlinkSync(metadataPath);
          console.log('ğŸ—‘ï¸  [Backup] Cleaned up temp metadata file');
        }

        // S3 ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
        await S3Service.cleanOldBackups('backups/', RETENTION_DAYS);
      } else {
        console.warn('âš ï¸  [Backup] S3 not configured - backups stored locally only (NOT recommended for Railway!)');
      }

      // ë¡œì»¬ ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (ë¡œì»¬ í™˜ê²½ì—ë§Œ í•„ìš”)
      if (!IS_RAILWAY) {
        this.cleanOldBackups();
      }

      const duration = (Date.now() - startTime) / 1000;
      console.log(`âœ… [Backup] Daily backup completed successfully in ${duration.toFixed(2)}s\n`);
    } catch (error) {
      const duration = (Date.now() - startTime) / 1000;
      const errorMessage = error instanceof Error ? error.message : 'Unknown error';

      console.error(`âŒ [Backup] Daily backup failed after ${duration.toFixed(2)}s:`, error);

      // ì‹¤íŒ¨ ë©”íƒ€ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œ
      if (S3Service.isConfigured()) {
        try {
          const failureMetadata = {
            timestamp,
            db_size_bytes: 0,
            db_size_mb: '0.00',
            uploads_size_bytes: 0,
            uploads_size_mb: '0.00',
            total_size_mb: '0.00',
            success: false,
            duration_seconds: duration.toFixed(2),
            error: errorMessage,
            environment: IS_RAILWAY ? 'railway' : 'local',
          };

          const metadataPath = IS_RAILWAY
            ? path.join('/tmp', `${timestamp}_metadata.json`)
            : path.join(BACKUP_DIR, `${timestamp}_metadata.json`);

          fs.writeFileSync(metadataPath, JSON.stringify(failureMetadata, null, 2));
          await this.uploadToS3(metadataPath, `${timestamp}_metadata.json`);

          if (IS_RAILWAY && fs.existsSync(metadataPath)) {
            fs.unlinkSync(metadataPath);
          }
        } catch (metaError) {
          console.error('âŒ [Backup] Failed to upload failure metadata:', metaError);
        }
      }

      throw error;
    }
  }

  /**
   * ë°±ì—… ëª©ë¡ ì¡°íšŒ (ê´€ë¦¬ìš©)
   */
  static listBackups(): Array<{ date: string; files: string[]; metadata?: any }> {
    try {
      if (!fs.existsSync(BACKUP_DIR)) {
        return [];
      }

      const files = fs.readdirSync(BACKUP_DIR);
      const backupsByDate: Record<string, string[]> = {};

      // ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
      files.forEach((file) => {
        const match = file.match(/^(\d{4}-\d{2}-\d{2})_/);
        if (match) {
          const date = match[1];
          if (!backupsByDate[date]) {
            backupsByDate[date] = [];
          }
          backupsByDate[date].push(file);
        }
      });

      // ë°°ì—´ë¡œ ë³€í™˜ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
      return Object.entries(backupsByDate)
        .map(([date, fileList]) => {
          const metadataFile = fileList.find((f) => f.endsWith('_metadata.json'));
          let metadata = null;

          if (metadataFile) {
            const metadataPath = path.join(BACKUP_DIR, metadataFile);
            metadata = JSON.parse(fs.readFileSync(metadataPath, 'utf-8'));
          }

          return { date, files: fileList, metadata };
        })
        .sort((a, b) => b.date.localeCompare(a.date)); // ìµœì‹ ìˆœ ì •ë ¬
    } catch (error) {
      console.error('âŒ [Backup] Failed to list backups:', error);
      return [];
    }
  }
}
